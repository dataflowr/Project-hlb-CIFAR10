{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as t\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download & Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "DATA_LOCATION = \"data.pt\"\n",
    "\n",
    "# if not os.path.exists(HYP['misc']['data_location']):\n",
    "\n",
    "CIFAR10_MEAN, CIFAR10_STD = [\n",
    "    torch.tensor(\n",
    "        [0.4913997551666284, 0.48215855929893703, 0.4465309133731618], device=DEVICE\n",
    "    ),\n",
    "    torch.tensor(\n",
    "        [0.24703225141799082, 0.24348516474564, 0.26158783926049628], device=DEVICE\n",
    "    ),\n",
    "]\n",
    "\n",
    "PREP_TRANSFORM = t.Compose(\n",
    "    [\n",
    "        TO_TENSOR := t.ToTensor(),\n",
    "        NORMALIZE := t.Normalize(CIFAR10_MEAN, CIFAR10_STD),\n",
    "    ]\n",
    ")\n",
    "\n",
    "TRAIN_TRANSFORM = t.Compose([PAD := t.Pad(BORDER := 4), PREP_TRANSFORM])\n",
    "\n",
    "cifar10 = torchvision.datasets.CIFAR10(\n",
    "    \"cifar10/\", download=True, train=True, transform=TRAIN_TRANSFORM\n",
    ")\n",
    "cifar10_eval = torchvision.datasets.CIFAR10(\n",
    "    \"cifar10/\", download=False, train=False, transform=PREP_TRANSFORM\n",
    ")\n",
    "\n",
    "\n",
    "data = {\"train\": cifar10, \"eval\": cifar10_eval}\n",
    "# torch.save(data, HYP['misc']['data_location'])\n",
    "\n",
    "# else:\n",
    "#     # This is effectively instantaneous, and takes us practically straight to where the dataloader-loaded dataset would be. :)\n",
    "#     # So as long as you run the above loading process once, and keep the file on the disc it's specified by default in the above\n",
    "#     # HYP dictionary, then we should be good. :)\n",
    "#     data = torch.load(HYP['misc']['data_location'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "#          Training Helpers            #\n",
    "########################################\n",
    "\n",
    "\n",
    "def sgd_optimizer(trainable_parameters, weight_decay: float):\n",
    "    return torch.optim.SGD(\n",
    "        trainable_parameters,\n",
    "        weight_decay,\n",
    "        **(SGD_DEFAULT_KWARGS := {\"nesterov\": True, \"momentum\": 0.9}),\n",
    "    )\n",
    "\n",
    "\n",
    "def piecewise_linear_scheduler(\n",
    "    optimizer: torch.optim.Optimizer, epochs: list[int], learning_rates: list[float]\n",
    "):\n",
    "    return torch.optim.lr_scheduler.SequentialLR(\n",
    "        optimizer,\n",
    "        schedulers=[\n",
    "            torch.optim.lr_scheduler.LinearLR(\n",
    "                optimizer,\n",
    "                start_factor=learning_rates[i],\n",
    "                end_factor=learning_rates[i + 1],\n",
    "            )\n",
    "            for i in range(len(learning_rates) - 1)\n",
    "        ],\n",
    "        milestones=epochs[1:-1],\n",
    "    )\n",
    "\n",
    "\n",
    "########################################\n",
    "#           Train and Eval             #\n",
    "########################################\n",
    "\n",
    "\n",
    "def train_test(\n",
    "    model: nn.Module,\n",
    "    criterion,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scheduler: torch.optim.lr_scheduler._LRScheduler,\n",
    "    train_set: torchvision.datasets.CIFAR10,\n",
    "    test_set: torchvision.datasets.CIFAR10,\n",
    "    num_epochs: int,\n",
    "    batch_size: int,\n",
    "    num_workers: int = 0,\n",
    "):\n",
    "    train_loader = DataLoader(\n",
    "        train_set, batch_size, shuffle=True, num_workers=num_workers\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_set, batch_size, shuffle=False, num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch, num_epochs))\n",
    "\n",
    "        train_epoch(model, criterion, optimizer, train_loader)\n",
    "        test_epoch(model, criterion, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "def train_epoch(\n",
    "    model: nn.Module,\n",
    "    criterion: nn.modules.loss._Loss,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    train_loader: DataLoader,\n",
    "):\n",
    "    model.train()\n",
    "\n",
    "    TRANSFORM = t.Compose(\n",
    "        [CROP := t.RandomCrop(IMAGE_SIZE := (32, 32)), FLIP := t.RandomVerticalFlip()]\n",
    "    )\n",
    "\n",
    "    train_correct = 0\n",
    "    train_loss = 0\n",
    "\n",
    "    size = 0\n",
    "    for i, (batch, targets) in enumerate(train_loader):\n",
    "        bs = targets.size(0)\n",
    "\n",
    "        batch = TRANSFORM(batch)\n",
    "        batch.to(DEVICE)\n",
    "\n",
    "        output = model(batch)\n",
    "        loss = criterion(output, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        train_correct += pred.eq(targets.view_as(pred)).sum().item()\n",
    "        train_loss += loss\n",
    "\n",
    "        size += bs\n",
    "\n",
    "        if i % 100 == 10:\n",
    "            print(\n",
    "                \"{:.2f}% Train - Loss: {:.4f} ; Acc: {:.2f}%\".format(\n",
    "                    100 * i / len(train_loader),\n",
    "                    train_loss / size,\n",
    "                    100 * train_correct / size,\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "def test_epoch(\n",
    "    model: nn.Module, criterion: nn.modules.loss._Loss, test_loader: DataLoader\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    test_correct = 0\n",
    "    test_loss = 0\n",
    "\n",
    "    size = 0\n",
    "    for batch, targets in test_loader:\n",
    "        bs = targets.size(0)\n",
    "\n",
    "        output = model(batch)\n",
    "\n",
    "        loss = criterion(output, targets)\n",
    "        test_loss += loss\n",
    "\n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        test_correct += pred.eq(targets.view_as(pred)).sum().item()\n",
    "\n",
    "        size += bs\n",
    "\n",
    "    print(\n",
    "        \"Train - Loss: {:.4f} ; Acc: {:.2f}%\".format(\n",
    "            test_loss / size, 100 * test_correct / size\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "#            Network Components             #\n",
    "#############################################\n",
    "\n",
    "\n",
    "class Cat(nn.Module):\n",
    "    def __init__(self, modules: OrderedDict[str, nn.Module]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        for name, module in modules.items():\n",
    "            self.add_module(name, module)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        return torch.cat([module(x) for module in self.children()])\n",
    "\n",
    "\n",
    "class Add(nn.Module):\n",
    "    def __init__(self, modules: OrderedDict[str, nn.Module]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        for name, module in modules.items():\n",
    "            self.add_module(name, module)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        return sum([module(x) for module in self.children()])\n",
    "\n",
    "\n",
    "class Id(nn.Module):\n",
    "    def forward(self, x: Tensor):\n",
    "        return x\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x: Tensor):\n",
    "        return x.view(x.size(0), x.size(1))\n",
    "\n",
    "\n",
    "class BatchNorm(nn.BatchNorm2d):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features,\n",
    "        weight_requires_grad=True,\n",
    "        bias_requires_grad=True,\n",
    "        weights_init=False,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(num_features, *args, **kwargs)\n",
    "\n",
    "        if weights_init:\n",
    "            self.weight.data.fill_(1.0)\n",
    "            self.bias.data.fill_(0.0)\n",
    "\n",
    "        self.weight.requires_grad = weight_requires_grad\n",
    "        self.bias.requires_grad = bias_requires_grad\n",
    "\n",
    "\n",
    "class Conv(nn.Conv2d):\n",
    "    def __init__(self, in_channels, out_channels, padding=None, *args, **kwargs):\n",
    "        kwargs = {\n",
    "            **kwargs,\n",
    "            **(\n",
    "                DEFAULT_CONV_KWARGS := {\n",
    "                    \"kernel_size\": 3,\n",
    "                    \"padding\": \"same\",\n",
    "                    \"bias\": False,\n",
    "                }\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        if padding is not None:\n",
    "            kwargs[\"padding\"] = padding\n",
    "\n",
    "        super().__init__(in_channels, out_channels, *args, **kwargs)\n",
    "\n",
    "\n",
    "class ResBlock(nn.Sequential):\n",
    "    def __init__(self, c_in: int, c_out: int, stride: int = 1) -> None:\n",
    "        bn1 = BatchNorm(c_in)\n",
    "        relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        branch = nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (\n",
    "                        \"conv1\",\n",
    "                        Conv(c_in, c_out, kernel_size=3, stride=stride, padding=1),\n",
    "                    ),\n",
    "                    (\"bn2\", BatchNorm(c_out)),\n",
    "                    (\"relu2\", nn.ReLU(inplace=True)),\n",
    "                    (\"conv2\", Conv(c_out, c_out, bias=False)),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        is_projection_needed = (stride != 1) or (c_in != c_out)\n",
    "\n",
    "        super().__init__(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (\"bn1\", bn1),\n",
    "                    (\"relu1\", relu1),\n",
    "                    (\n",
    "                        \"res\",\n",
    "                        Add(\n",
    "                            OrderedDict(\n",
    "                                [\n",
    "                                    (\n",
    "                                        (\n",
    "                                            \"conv3\",\n",
    "                                            Conv(\n",
    "                                                c_in,\n",
    "                                                c_out,\n",
    "                                                kernel_size=1,\n",
    "                                                stride=stride,\n",
    "                                                padding=1,\n",
    "                                                bias=False,\n",
    "                                            ),\n",
    "                                        )\n",
    "                                        if is_projection_needed\n",
    "                                        else (\"id\", Id())\n",
    "                                    ),\n",
    "                                    (\"branch\", branch),\n",
    "                                ]\n",
    "                            )\n",
    "                        ),\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "#############################################\n",
    "#            Network Architercture          #\n",
    "#############################################\n",
    "\n",
    "\n",
    "class DawnNet(nn.Sequential):\n",
    "    def __init__(\n",
    "        self, c=64, Block=ResBlock, prep_bn_relu=False, concat_pool=False, **kw\n",
    "    ) -> None:\n",
    "        if isinstance(c, int):\n",
    "            c = [c, 2 * c, 4 * c, 4 * c]\n",
    "\n",
    "        prep = nn.Sequential(\n",
    "            OrderedDict([(\"conv\", Conv(in_channels=3, out_channels=c[0], bias=False))])\n",
    "        )\n",
    "\n",
    "        if prep_bn_relu:\n",
    "            prep.add_module(\"bn\", BatchNorm(c[0], **kw))\n",
    "            prep.add_module(\"relu\", nn.ReLU(True))\n",
    "\n",
    "        classifier_pool = (\n",
    "            Cat(\n",
    "                OrderedDict(\n",
    "                    [(\"maxpool\", nn.MaxPool2d(4)), (\"avgpool\", nn.AvgPool2d(4))]\n",
    "                )\n",
    "            )\n",
    "            if concat_pool\n",
    "            else nn.MaxPool2d(4)\n",
    "        )\n",
    "\n",
    "        super().__init__(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (\"prep\", prep),\n",
    "                    (\n",
    "                        \"layer1\",\n",
    "                        nn.Sequential(\n",
    "                            OrderedDict(\n",
    "                                [\n",
    "                                    (\"block0\", Block(c[0], c[0], **kw)),\n",
    "                                    (\"block1\", Block(c[0], c[0], **kw)),\n",
    "                                ]\n",
    "                            )\n",
    "                        ),\n",
    "                    ),\n",
    "                    (\n",
    "                        \"layer2\",\n",
    "                        nn.Sequential(\n",
    "                            OrderedDict(\n",
    "                                [\n",
    "                                    (\"block0\", Block(c[0], c[1], stride=2, **kw)),\n",
    "                                    (\"block1\", Block(c[1], c[1], **kw)),\n",
    "                                ]\n",
    "                            )\n",
    "                        ),\n",
    "                    ),\n",
    "                    (\n",
    "                        \"layer3\",\n",
    "                        nn.Sequential(\n",
    "                            OrderedDict(\n",
    "                                [\n",
    "                                    (\"block0\", Block(c[1], c[2], stride=2, **kw)),\n",
    "                                    (\"block1\", Block(c[2], c[2], **kw)),\n",
    "                                ]\n",
    "                            )\n",
    "                        ),\n",
    "                    ),\n",
    "                    (\n",
    "                        \"layer4\",\n",
    "                        nn.Sequential(\n",
    "                            OrderedDict(\n",
    "                                [\n",
    "                                    (\"block0\", Block(c[2], c[3], stride=2, **kw)),\n",
    "                                    (\"block1\", Block(c[3], c[3], **kw)),\n",
    "                                ]\n",
    "                            )\n",
    "                        ),\n",
    "                    ),\n",
    "                    (\n",
    "                        \"final\",\n",
    "                        nn.Sequential(\n",
    "                            OrderedDict(\n",
    "                                [\n",
    "                                    (\"pool\", classifier_pool),\n",
    "                                    (\"flatten\", Flatten()),\n",
    "                                    (\n",
    "                                        \"linear\",\n",
    "                                        nn.Linear(\n",
    "                                            (2 * c[3] if concat_pool else c[3]),\n",
    "                                            10,\n",
    "                                            bias=True,\n",
    "                                        ),\n",
    "                                    ),\n",
    "                                ]\n",
    "                            )\n",
    "                        ),\n",
    "                    ),\n",
    "                    (\"logits\", Id()),\n",
    "                ]\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Post 1: Baseline](https://www.myrtle.ai/2018/09/24/how_to_train_your_resnet_1/) - DAWNbench baseline + no initial bn-relu+ efficient dataloading/augmentation, 1 dataloader process (301s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "10%\n",
      "Train - Loss: 0.0211 ; Acc: 9.87%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m scheduler \u001b[39m=\u001b[39m piecewise_linear_scheduler(optimizer, \n\u001b[1;32m      6\u001b[0m                                        EPOCHS\u001b[39m:=\u001b[39m[\u001b[39m0\u001b[39m, \u001b[39m15\u001b[39m, \u001b[39m30\u001b[39m, NUM_EPOCHS\u001b[39m:=\u001b[39m\u001b[39m35\u001b[39m], \n\u001b[1;32m      7\u001b[0m                                        LR\u001b[39m:=\u001b[39m[\u001b[39m0\u001b[39m, \u001b[39m0.1\u001b[39m, \u001b[39m0.005\u001b[39m, \u001b[39m0\u001b[39m])\n\u001b[1;32m      9\u001b[0m \u001b[39m# print(net.final)\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m train_test(net, criterion, optimizer, scheduler, data[\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m], data[\u001b[39m\"\u001b[39;49m\u001b[39meval\u001b[39;49m\u001b[39m\"\u001b[39;49m], NUM_EPOCHS, BATCHSIZE, NUM_WORKERS\u001b[39m:=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[3], line 57\u001b[0m, in \u001b[0;36mtrain_test\u001b[0;34m(model, criterion, optimizer, scheduler, train_set, test_set, num_epochs, batch_size, num_workers)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m     55\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(epoch))\n\u001b[0;32m---> 57\u001b[0m     train_epoch(model, criterion, optimizer, train_loader)\n\u001b[1;32m     58\u001b[0m     test_epoch(model, criterion, test_loader)\n\u001b[1;32m     59\u001b[0m     scheduler\u001b[39m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[3], line 89\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, criterion, optimizer, train_loader)\u001b[0m\n\u001b[1;32m     85\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, targets)\n\u001b[1;32m     87\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 89\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     90\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     92\u001b[0m pred \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mmax(\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net = DawnNet()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = sgd_optimizer(TRAINABLE_PARAMETERS:=net.parameters(), weight_decay=5e-4*(BATCHSIZE := 128))\n",
    "scheduler = piecewise_linear_scheduler(optimizer, \n",
    "                                       EPOCHS:=[0, 15, 30, NUM_EPOCHS:=35], \n",
    "                                       LR:=[0, 0.1, 0.005, 0])\n",
    "\n",
    "# print(net.final)\n",
    "train_test(net, criterion, optimizer, scheduler, data[\"train\"], data[\"eval\"], NUM_EPOCHS, BATCHSIZE, NUM_WORKERS:=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project-hlb-CIFAR10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHSIZE = 512\n",
    "BIAS_SCALER = 32\n",
    "\n",
    "# To replicate the ~95.77% accuracy in 188 seconds runs, simply change the base_depth from 64->128 and the num_epochs from 10->80\n",
    "HYP = {\n",
    "    'opt': {\n",
    "        'bias_lr':        1.15 * 1.35 * 1. * BIAS_SCALER/BATCHSIZE, # TODO: How we're expressing this information feels somewhat clunky, is there maybe a better way to do this? :'))))\n",
    "        'non_bias_lr':    1.15 * 1.35 * 1. / BATCHSIZE,\n",
    "        'bias_decay':     .85 * 4.8e-4 * BATCHSIZE/BIAS_SCALER,\n",
    "        'non_bias_decay': .85 * 4.8e-4 * BATCHSIZE,\n",
    "        'scaling_factor': 1./10,\n",
    "        'percent_start': .2,\n",
    "    },\n",
    "    'net': {\n",
    "        'whitening': {\n",
    "            'kernel_size': 2,\n",
    "            'num_examples': 50000,\n",
    "        },\n",
    "        'batch_norm_momentum': .8,\n",
    "        'cutout_size': 0,\n",
    "        'pad_amount': 3,\n",
    "        'base_depth': 64 ## This should be a factor of 8 in some way to stay tensor core friendly\n",
    "    },\n",
    "    'misc': {\n",
    "        'ema': {\n",
    "            'epochs': 2,\n",
    "            'decay_base': .986,\n",
    "            'every_n_steps': 2,\n",
    "        },\n",
    "        'train_epochs': 10,\n",
    "        'device': DEVICE,\n",
    "        'data_location': DATA_LOCATION,\n",
    "    }\n",
    "}\n",
    "\n",
    "SCALER = 2. ## You can play with this on your own if you want, for the first beta I wanted to keep things simple (for now) and leave it out of the hyperparams dict\n",
    "DEPTHS = {\n",
    "    'init':   round(SCALER**-1*HYP['net']['base_depth']), # 64  w/ scaler at base value\n",
    "    'block1': round(SCALER**1*HYP['net']['base_depth']), # 128 w/ scaler at base value\n",
    "    'block2': round(SCALER**2*HYP['net']['base_depth']), # 256 w/ scaler at base value\n",
    "    'block3': round(SCALER**3*HYP['net']['base_depth']), # 512 w/ scaler at base value\n",
    "    'num_classes': 10\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Init Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patches(x, patch_shape=(3, 3), dtype=torch.float32):\n",
    "    # TODO: Annotate\n",
    "    c, (h, w) = x.shape[1], patch_shape\n",
    "    return x.unfold(2,h,1).unfold(3,w,1).transpose(1,3).reshape(-1,c,h,w).to(dtype) # TODO: Annotate?\n",
    "\n",
    "def get_whitening_parameters(patches):\n",
    "    # TODO: Let's annotate this, please! :'D / D':\n",
    "    n,c,h,w = patches.shape\n",
    "    est_covariance = torch.cov(patches.view(n, c*h*w).t())\n",
    "    eigenvalues, eigenvectors = torch.linalg.eigh(est_covariance, UPLO='U') # this is the same as saying we want our eigenvectors, with the specification that the matrix be an upper triangular matrix (instead of a lower-triangular matrix)\n",
    "    return eigenvalues.flip(0).view(-1, 1, 1, 1), eigenvectors.t().reshape(c*h*w,c,h,w).flip(0)\n",
    "\n",
    "# Run this over the training set to calculate the patch statistics, then set the initial convolution as a non-learnable 'whitening' layer\n",
    "def init_whitening_conv(layer, train_set=None, num_examples=None, previous_block_data=None, pad_amount=0, freeze=True, whiten_splits=None):\n",
    "    if train_set is not None and previous_block_data is None:\n",
    "        if pad_amount > 0:\n",
    "            previous_block_data = train_set[:num_examples,:,pad_amount:-pad_amount,pad_amount:-pad_amount] # if it's none, we're at the beginning of our network.\n",
    "        else:\n",
    "            previous_block_data = train_set[:num_examples,:,:,:]\n",
    "    if whiten_splits is None:\n",
    "         previous_block_data_split = [previous_block_data] # list of length 1 so we can reuse the splitting code down below\n",
    "    else:\n",
    "         previous_block_data_split = previous_block_data.split(whiten_splits, dim=0)\n",
    "\n",
    "    eigenvalue_list, eigenvector_list = [], []\n",
    "    for data_split in previous_block_data_split:\n",
    "        eigenvalues, eigenvectors = get_whitening_parameters(get_patches(data_split, patch_shape=layer.weight.data.shape[2:])) # center crop to remove padding\n",
    "        eigenvalue_list.append(eigenvalues)\n",
    "        eigenvector_list.append(eigenvectors)\n",
    "\n",
    "    eigenvalues = torch.stack(eigenvalue_list, dim=0).mean(0)\n",
    "    eigenvectors = torch.stack(eigenvector_list, dim=0).mean(0)\n",
    "    # for some reason, the eigenvalues and eigenvectors seem to come out all in float32 for this? ! ?! ?!?!?!? :'(((( </3\n",
    "    set_whitening_conv(layer, eigenvalues.to(dtype=layer.weight.dtype), eigenvectors.to(dtype=layer.weight.dtype), freeze=freeze)\n",
    "    data = layer(previous_block_data.to(dtype=layer.weight.dtype))\n",
    "    return data\n",
    "\n",
    "def set_whitening_conv(conv_layer, eigenvalues, eigenvectors, eps=1e-2, freeze=True):\n",
    "    shape = conv_layer.weight.data.shape\n",
    "    conv_layer.weight.data[-eigenvectors.shape[0]:, :, :, :] = (eigenvectors/torch.sqrt(eigenvalues+eps))[-shape[0]:, :, :, :]\n",
    "    ## We don't want to train this, since this is implicitly whitening over the whole dataset\n",
    "    ## For more info, see David Page's original blogposts (link in the README.md as of this commit.)\n",
    "    if freeze: \n",
    "        conv_layer.weight.requires_grad = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "#         (EXTRA) Network Components        #\n",
    "#############################################\n",
    "\n",
    "# can hack any changes to each residual group that you want directly in here\n",
    "class ConvGroup(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out, residual, short, pool, se):\n",
    "        super().__init__()\n",
    "        self.short = short\n",
    "        self.pool = pool\n",
    "        self.se = se\n",
    "\n",
    "        self.residual = residual\n",
    "        self.channels_in = channels_in\n",
    "        self.channels_out = channels_out\n",
    "\n",
    "        self.conv1 = Conv(channels_in, channels_out)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.norm1 = BatchNorm(channels_out)\n",
    "        self.activ = nn.GELU()          \n",
    "\n",
    "        if not short:\n",
    "            self.conv2 = Conv(channels_out, channels_out)\n",
    "            self.conv3 = Conv(channels_out, channels_out)\n",
    "            self.norm2 = BatchNorm(channels_out)\n",
    "            self.norm3 = BatchNorm(channels_out)\n",
    "\n",
    "            self.se1 = nn.Linear(channels_out, channels_out//16)\n",
    "            self.se2 = nn.Linear(channels_out//16, channels_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        if self.pool:\n",
    "            x = self.pool1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.activ(x)\n",
    "        if self.short: # layer 2 doesn't necessarily need the residual, so we just return it.\n",
    "            return x\n",
    "        residual = x\n",
    "        if self.se:\n",
    "            mult = torch.sigmoid(self.se2(self.activ(self.se1(torch.mean(residual, dim=(2,3)))))).unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.activ(x)\n",
    "        x = self.conv3(x)\n",
    "        \n",
    "        if self.se:\n",
    "            x = x * mult\n",
    "\n",
    "        x = self.norm3(x)\n",
    "        x = self.activ(x)\n",
    "        x = x + residual # haiku\n",
    "\n",
    "        return x\n",
    "\n",
    "# Set to 1 for now just to debug a few things....\n",
    "class TemperatureScaler(nn.Module):\n",
    "    def __init__(self, init_val):\n",
    "        super().__init__()\n",
    "        self.scaler = torch.tensor(init_val)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x.float() ## save precision for the gradients in the backwards pass\n",
    "                  ## I personally believe from experience that this is important\n",
    "                  ## for a few reasons. I believe this is the main functional difference between\n",
    "                  ## my implementation, and David's implementation...\n",
    "        return x.mul(self.scaler)\n",
    "\n",
    "class FastGlobalMaxPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Previously was chained torch.max calls.\n",
    "        # requires less time than AdaptiveMax2dPooling -- about ~.3s for the entire run, in fact (which is pretty significant! :O :D :O :O <3 <3 <3 <3)\n",
    "        return torch.amax(x, dim=(2,3)) # Global maximum pooling\n",
    "    \n",
    "#############################################\n",
    "#            Network Definition             #\n",
    "#############################################\n",
    "    \n",
    "class SpeedyResNet(nn.Module):\n",
    "    def __init__(self, network_dict):\n",
    "        super().__init__()\n",
    "        self.net_dict = network_dict # flexible, defined in the make_net function\n",
    "\n",
    "    # This allows you to customize/change the execution order of the network as needed.\n",
    "    def forward(self, x):\n",
    "        if not self.training:\n",
    "            x = torch.cat((x, torch.flip(x, (-1,))))\n",
    "        x = self.net_dict['initial_block']['whiten'](x)\n",
    "        x = self.net_dict['initial_block']['project'](x)\n",
    "        x = self.net_dict['initial_block']['norm'](x)\n",
    "        x = self.net_dict['initial_block']['activation'](x)\n",
    "        x = self.net_dict['residual1'](x)\n",
    "        x = self.net_dict['residual2'](x)\n",
    "        x = self.net_dict['residual3'](x)\n",
    "        x = self.net_dict['pooling'](x)\n",
    "        x = self.net_dict['linear'](x)\n",
    "        x = self.net_dict['temperature'](x)\n",
    "        if not self.training:\n",
    "            # Average the predictions from the lr-flipped inputs during eval\n",
    "            orig, flipped = x.split(x.shape[0]//2, dim=0)\n",
    "            x = .5 * orig + .5 * flipped\n",
    "        return x\n",
    "    \n",
    "def make_net():\n",
    "    # TODO: A way to make this cleaner??\n",
    "    # Note, you have to specify any arguments overlapping with defaults (i.e. everything but in/out depths) as kwargs so that they are properly overridden (TODO cleanup somehow?)\n",
    "    whiten_conv_depth = 3*HYP['net']['whitening']['kernel_size']**2\n",
    "    network_dict = nn.ModuleDict({\n",
    "        'initial_block': nn.ModuleDict({\n",
    "            'whiten': Conv(3, whiten_conv_depth, kernel_size=HYP['net']['whitening']['kernel_size'], padding=0),\n",
    "            'project': Conv(whiten_conv_depth, DEPTHS['init'], kernel_size=1),\n",
    "            'norm': BatchNorm(DEPTHS['init'], weight=False),\n",
    "            'activation': nn.GELU(),\n",
    "        }),\n",
    "        'residual1': ConvGroup(DEPTHS['init'], DEPTHS['block1'], residual=True, short=False, pool=True, se=True),\n",
    "        'residual2': ConvGroup(DEPTHS['block1'], DEPTHS['block2'], residual=True, short=True, pool=True, se=True),\n",
    "        'residual3': ConvGroup(DEPTHS['block2'], DEPTHS['block3'], residual=True, short=False, pool=True, se=True),\n",
    "        'pooling': FastGlobalMaxPooling(),\n",
    "        'linear': nn.Linear(DEPTHS['block3'], DEPTHS['num_classes'], bias=False),\n",
    "        'temperature': TemperatureScaler(HYP['opt']['scaling_factor'])\n",
    "    })\n",
    "\n",
    "    net = SpeedyResNet(network_dict)\n",
    "    net = net.to(HYP['misc']['device'])\n",
    "    net = net.to(memory_format=torch.channels_last) # to appropriately use tensor cores/avoid thrash while training\n",
    "    net.train()\n",
    "    net.half() # Convert network to half before initializing the initial whitening layer.\n",
    "\n",
    "    ## Initialize the whitening convolution\n",
    "    with torch.no_grad():\n",
    "        # Initialize the first layer to be fixed weights that whiten the expected input values of the network be on the unit HYPersphere. (i.e. their...average vector length is 1.?, IIRC)\n",
    "        init_whitening_conv(net.net_dict['initial_block']['whiten'],\n",
    "                            data['train']['images'].index_select(0, torch.randperm(data['train']['images'].shape[0], device=data['train']['images'].device)),\n",
    "                            num_examples=HYP['net']['whitening']['num_examples'],\n",
    "                            pad_amount=HYP['net']['pad_amount'],\n",
    "                            whiten_splits=5000) ## Hardcoded for now while we figure out the optimal whitening number\n",
    "                                                ## If you're running out of memory (OOM) feel free to decrease this, but\n",
    "                                                ## the index lookup in the dataloader may give you some trouble depending\n",
    "                                                ## upon exactly how memory-limited you are\n",
    "\n",
    "    return net"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing (EXTRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is actually (I believe) a pretty clean implementation of how to do something like this, since shifted-square masks unique to each depth-channel can actually be rather\n",
    "## tricky in practice. That said, if there's a better way, please do feel free to submit it! This can be one of the harder parts of the code to understand (though I personally get\n",
    "## stuck on the fold/unfold process for the lower-level convolution calculations.\n",
    "def make_random_square_masks(inputs, mask_size):\n",
    "    ##### TODO: Double check that this properly covers the whole range of values. :'( :')\n",
    "    if mask_size == 0:\n",
    "        return None # no need to cutout or do anything like that since the patch_size is set to 0\n",
    "    is_even = int(mask_size % 2 == 0)\n",
    "    in_shape = inputs.shape\n",
    "\n",
    "    # seed centers of squares to cutout boxes from, in one dimension each\n",
    "    mask_center_y = torch.empty(in_shape[0], dtype=torch.long, device=inputs.device).random_(mask_size//2-is_even, in_shape[-2]-mask_size//2-is_even)\n",
    "    mask_center_x = torch.empty(in_shape[0], dtype=torch.long, device=inputs.device).random_(mask_size//2-is_even, in_shape[-1]-mask_size//2-is_even)\n",
    "\n",
    "    # measure distance, using the center as a reference point\n",
    "    to_mask_y_dists = torch.arange(in_shape[-2], device=inputs.device).view(1, 1, in_shape[-2], 1) - mask_center_y.view(-1, 1, 1, 1)\n",
    "    to_mask_x_dists = torch.arange(in_shape[-1], device=inputs.device).view(1, 1, 1, in_shape[-1]) - mask_center_x.view(-1, 1, 1, 1)\n",
    "\n",
    "    to_mask_y = (to_mask_y_dists >= (-(mask_size // 2) + is_even)) * (to_mask_y_dists <= mask_size // 2)\n",
    "    to_mask_x = (to_mask_x_dists >= (-(mask_size // 2) + is_even)) * (to_mask_x_dists <= mask_size // 2)\n",
    "\n",
    "    final_mask = to_mask_y * to_mask_x ## Turn (y by 1) and (x by 1) boolean masks into (y by x) masks through multiplication. Their intersection is square, hurray! :D\n",
    "\n",
    "    return final_mask\n",
    "\n",
    "def batch_cutout(inputs, patch_size):\n",
    "    with torch.no_grad():\n",
    "        cutout_batch_mask = make_random_square_masks(inputs, patch_size)\n",
    "        if cutout_batch_mask is None:\n",
    "            return inputs # if the mask is None, then that's because the patch size was set to 0 and we will not be using cutout today.\n",
    "        # TODO: Could be fused with the crop operation for sheer speeeeeds. :D <3 :))))\n",
    "        cutout_batch = torch.where(cutout_batch_mask, torch.zeros_like(inputs), inputs)\n",
    "        return cutout_batch\n",
    "    \n",
    "def batch_crop(inputs, crop_size):\n",
    "    with torch.no_grad():\n",
    "        crop_mask_batch = make_random_square_masks(inputs, crop_size)\n",
    "        cropped_batch = torch.masked_select(inputs, crop_mask_batch).view(inputs.shape[0], inputs.shape[1], crop_size, crop_size)\n",
    "        return cropped_batch\n",
    "\n",
    "def batch_flip_lr(batch_images, flip_chance=.5):\n",
    "    with torch.no_grad():\n",
    "        # TODO: Is there a more elegant way to do this? :') :'((((\n",
    "        return torch.where(torch.rand_like(batch_images[:, 0, 0, 0].view(-1, 1, 1, 1)) < flip_chance, torch.flip(batch_images, (-1,)), batch_images)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Helpers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "class NetworkEMA(nn.Module):\n",
    "    def __init__(self, net, decay):\n",
    "        super().__init__() # init the parent module so this module is registered properly\n",
    "        self.net_ema = copy.deepcopy(net).eval().requires_grad_(False) # copy the model\n",
    "        self.decay = decay ## you can update/hack this as necessary for update scheduling purposes :3\n",
    "\n",
    "    def update(self, current_net):\n",
    "        with torch.no_grad():\n",
    "            for ema_net_parameter, incoming_net_parameter in zip(self.net_ema.state_dict().values(), current_net.state_dict().values()): # potential bug: assumes that the network architectures don't change during training (!!!!)\n",
    "                if incoming_net_parameter.dtype in (torch.half, torch.float):\n",
    "                    ema_net_parameter.mul_(self.decay).add_(incoming_net_parameter.detach().mul(1. - self.decay)) # update the ema values in place, similar to how optimizer momentum is coded\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        with torch.no_grad():\n",
    "            return self.net_ema(inputs)\n",
    "\n",
    "# TODO: Could we jit this in the (more distant) future? :)\n",
    "@torch.no_grad()\n",
    "def get_batches(data_dict, key, batchsize):\n",
    "    num_epoch_examples = len(data_dict[key]['images'])\n",
    "    shuffled = torch.randperm(num_epoch_examples, device='cuda')\n",
    "    crop_size = 32\n",
    "    ## Here, we prep the dataset by applying all data augmentations in batches ahead of time before each epoch, then we return an iterator below\n",
    "    ## that iterates in chunks over with a random derangement (i.e. shuffled indices) of the individual examples. So we get perfectly-shuffled\n",
    "    ## batches (which skip the last batch if it's not a full batch), but everything seems to be (and hopefully is! :D) properly shuffled. :)\n",
    "    if key == 'train':\n",
    "        images = batch_crop(data_dict[key]['images'], crop_size) # TODO: hardcoded image size for now?\n",
    "        images = batch_flip_lr(images)\n",
    "        images = batch_cutout(images, patch_size=HYP['net']['cutout_size'])\n",
    "    else:\n",
    "        images = data_dict[key]['images']\n",
    "\n",
    "    # Send the images to an (in beta) channels_last to help improve tensor core occupancy (and reduce NCHW <-> NHWC thrash) during training\n",
    "    images = images.to(memory_format=torch.channels_last)\n",
    "    for idx in range(num_epoch_examples // batchsize):\n",
    "        if not (idx+1)*batchsize > num_epoch_examples: ## Use the shuffled randperm to assemble individual items into a minibatch\n",
    "            yield images.index_select(0, shuffled[idx*batchsize:(idx+1)*batchsize]), \\\n",
    "                  data_dict[key]['targets'].index_select(0, shuffled[idx*batchsize:(idx+1)*batchsize]) ## Each item is only used/accessed by the network once per epoch. :D\n",
    "\n",
    "\n",
    "def init_split_parameter_dictionaries(network):\n",
    "    params_non_bias = {'params': [], 'lr': HYP['opt']['non_bias_lr'], 'momentum': .85, 'nesterov': True, 'weight_decay': HYP['opt']['non_bias_decay']}\n",
    "    params_bias     = {'params': [], 'lr': HYP['opt']['bias_lr'],     'momentum': .85, 'nesterov': True, 'weight_decay': HYP['opt']['bias_decay']}\n",
    "\n",
    "    for name, p in network.named_parameters():\n",
    "        if p.requires_grad:\n",
    "            if 'bias' in name:\n",
    "                params_bias['params'].append(p)\n",
    "            else:\n",
    "                params_non_bias['params'].append(p)\n",
    "    return params_non_bias, params_bias\n",
    "\n",
    "\n",
    "## Hey look, it's the soft-targets/label-smoothed loss! Native to PyTorch. Now, _that_ is pretty cool, and simplifies things a lot, to boot! :D :)\n",
    "loss_fn = nn.CrossEntropyLoss(label_smoothing=0.2, reduction='none')\n",
    "\n",
    "logging_columns_list = ['epoch', 'train_loss', 'val_loss', 'train_acc', 'val_acc', 'ema_val_acc', 'total_time_seconds']\n",
    "# define the printing function and print the column heads\n",
    "def print_training_details(columns_list, separator_left='|  ', separator_right='  ', final=\"|\", column_heads_only=False, is_final_entry=False):\n",
    "    print_string = \"\"\n",
    "    if column_heads_only:\n",
    "        for column_head_name in columns_list:\n",
    "            print_string += separator_left + column_head_name + separator_right\n",
    "        print_string += final\n",
    "        print('-'*(len(print_string))) # print the top bar\n",
    "        print(print_string)\n",
    "        print('-'*(len(print_string))) # print the bottom bar\n",
    "    else:\n",
    "        for column_value in columns_list:\n",
    "            print_string += separator_left + column_value + separator_right\n",
    "        print_string += final\n",
    "        print(print_string)\n",
    "    if is_final_entry:\n",
    "        print('-'*(len(print_string))) # print the final output bar\n",
    "\n",
    "print_training_details(logging_columns_list, column_heads_only=True) ## print out the training column heads before we print the actual content for each run.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train & Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Initializing constants for the whole run.\n",
    "    net_ema = None ## Reset any existing network emas, we want to have _something_ to check for existence so we can initialize the EMA right from where the network is during training\n",
    "                   ## (as opposed to initializing the network_ema from the randomly-initialized starter network, then forcing it to play catch-up all of a sudden in the last several epochs)\n",
    "\n",
    "    total_time_seconds = 0.\n",
    "    current_steps = 0.\n",
    "    \n",
    "    # TODO: Doesn't currently account for partial epochs really (since we're not doing \"real\" epochs across the whole BATCHSIZE)....\n",
    "    num_steps_per_epoch      = len(data['train']['images']) // BATCHSIZE\n",
    "    total_train_steps        = num_steps_per_epoch * HYP['misc']['train_epochs']\n",
    "    ema_epoch_start          = HYP['misc']['train_epochs'] - HYP['misc']['ema']['epochs']\n",
    "    num_cooldown_before_freeze_steps = 0\n",
    "    num_low_lr_steps_for_ema = HYP['misc']['ema']['epochs'] * num_steps_per_epoch\n",
    "\n",
    "    ## I believe this wasn't logged, but the EMA update power is adjusted by being raised to the power of the number of \"every n\" steps\n",
    "    ## to somewhat accomodate for whatever the expected information intake rate is. The tradeoff I believe, though, is that this is to some degree noisier as we\n",
    "    ## are intaking fewer samples of our distribution-over-time, with a higher individual weight each. This can be good or bad depending upon what we want.\n",
    "    projected_ema_decay_val  = HYP['misc']['ema']['decay_base'] ** HYP['misc']['ema']['every_n_steps']\n",
    "\n",
    "    # Adjust pct_start based upon how many epochs we need to finetune the ema at a low lr for\n",
    "    pct_start = HYP['opt']['percent_start'] * (total_train_steps/(total_train_steps - num_low_lr_steps_for_ema))\n",
    "\n",
    "    # Get network\n",
    "    net = make_net()\n",
    "\n",
    "    ## Stowing the creation of these into a helper function to make things a bit more readable....\n",
    "    non_bias_params, bias_params = init_split_parameter_dictionaries(net)\n",
    "\n",
    "    # One optimizer for the regular network, and one for the biases. This allows us to use the superconvergence onecycle training policy for our networks....\n",
    "    opt = torch.optim.SGD(**non_bias_params)\n",
    "    opt_bias = torch.optim.SGD(**bias_params)\n",
    "\n",
    "    #opt = torch.optim.SGD(**non_bias_params)\n",
    "    #opt_bias = torch.optim.SGD(**bias_params)\n",
    "\n",
    "    ## Not the most intuitive, but this basically takes us from ~0 to max_lr at the point pct_start, then down to .1 * max_lr at the end (since 1e16 * 1e-15 = .1 --\n",
    "    ##   This quirk is because the final lr value is calculated from the starting lr value and not from the maximum lr value set during training)\n",
    "    initial_div_factor = 1e16 # basically to make the initial lr ~0 or so :D\n",
    "    final_lr_ratio = .135\n",
    "    lr_sched      = torch.optim.lr_scheduler.OneCycleLR(opt,  max_lr=non_bias_params['lr'], pct_start=pct_start, div_factor=initial_div_factor, final_div_factor=1./(initial_div_factor*final_lr_ratio), total_steps=total_train_steps-num_low_lr_steps_for_ema, anneal_strategy='linear', cycle_momentum=False)\n",
    "    lr_sched_bias = torch.optim.lr_scheduler.OneCycleLR(opt_bias, max_lr=bias_params['lr'], pct_start=pct_start, div_factor=initial_div_factor, final_div_factor=1./(initial_div_factor*final_lr_ratio), total_steps=total_train_steps-num_low_lr_steps_for_ema, anneal_strategy='linear', cycle_momentum=False)\n",
    "\n",
    "    ## For accurately timing GPU code\n",
    "    starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "    ## There's another repository that's mainly reorganized David's code while still maintaining some of the functional structure, and it\n",
    "    ## has a timing feature too, but there's no synchronizes so I suspect the times reported are much faster than they may be in actuality\n",
    "    ## due to some of the quirks of timing GPU operations.\n",
    "    torch.cuda.synchronize() ## clean up any pre-net setup operations\n",
    "    \n",
    "\n",
    "    if True: ## Sometimes we need a conditional/for loop here, this is placed to save the trouble of needing to indent\n",
    "        for epoch in range(HYP['misc']['train_epochs']):\n",
    "          #################\n",
    "          # Training Mode #\n",
    "          #################\n",
    "          torch.cuda.synchronize()\n",
    "          starter.record()\n",
    "          net.train()\n",
    "\n",
    "          loss_train = None\n",
    "          accuracy_train = None\n",
    "\n",
    "          for epoch_step, (inputs, targets) in enumerate(get_batches(data, key='train', batchsize=BATCHSIZE)):\n",
    "              ## Run everything through the network\n",
    "              outputs = net(inputs)\n",
    "              \n",
    "              loss_scale_scaler = 1./16 # Hardcoded for now, preserves some accuracy during the loss summing process, balancing out its regularization effects\n",
    "              ## If you want to add other losses or hack around with the loss, you can do that here.\n",
    "              loss = loss_fn(outputs, targets).mul(loss_scale_scaler).sum().div(loss_scale_scaler) ## Note, as noted in the original blog posts, the summing here does a kind of loss scaling\n",
    "                                                     ## (and is thus BATCHSIZE dependent as a result). This can be somewhat good or bad, depending...\n",
    "\n",
    "              # we only take the last-saved accs and losses from train\n",
    "              if epoch_step % 50 == 0:\n",
    "                  train_acc = (outputs.detach().argmax(-1) == targets).float().mean().item()\n",
    "                  train_loss = loss.detach().cpu().item()/BATCHSIZE\n",
    "\n",
    "              loss.backward()\n",
    "\n",
    "              ## Step for each optimizer, in turn.\n",
    "              opt.step()\n",
    "              opt_bias.step()\n",
    "\n",
    "              if current_steps < total_train_steps - num_low_lr_steps_for_ema - 1: # the '-1' is because the lr scheduler tends to overshoot (even below 0 if the final lr is ~0) on the last step for some reason.\n",
    "                  # We only want to step the lr_schedulers while we have training steps to consume. Otherwise we get a not-so-friendly error from PyTorch\n",
    "                  lr_sched.step()\n",
    "                  lr_sched_bias.step()\n",
    "\n",
    "              ## Using 'set_to_none' I believe is slightly faster (albeit riskier w/ funky gradient update workflows) than under the default 'set to zero' method\n",
    "              opt.zero_grad(set_to_none=True)\n",
    "              opt_bias.zero_grad(set_to_none=True)\n",
    "              current_steps += 1\n",
    "\n",
    "              if epoch >= ema_epoch_start and current_steps % HYP['misc']['ema']['every_n_steps'] == 0:          \n",
    "                  ## Initialize the ema from the network at this point in time if it does not already exist.... :D\n",
    "                  if net_ema is None or epoch_step < num_cooldown_before_freeze_steps: # don't snapshot the network yet if so!\n",
    "                      net_ema = NetworkEMA(net, decay=projected_ema_decay_val)\n",
    "                      continue\n",
    "                  net_ema.update(net)\n",
    "          ender.record()\n",
    "          torch.cuda.synchronize()\n",
    "          total_time_seconds += 1e-3 * starter.elapsed_time(ender)\n",
    "\n",
    "          ####################\n",
    "          # Evaluation  Mode #\n",
    "          ####################\n",
    "          net.eval()\n",
    "\n",
    "          EVAL_BATCHSIZE = 1000\n",
    "          assert data['eval']['images'].shape[0] % EVAL_BATCHSIZE == 0, \"Error: The eval BATCHSIZE must evenly divide the eval dataset (for now, we don't have drop_remainder implemented yet).\"\n",
    "          loss_list_val, acc_list, acc_list_ema = [], [], []\n",
    "          \n",
    "          with torch.no_grad():\n",
    "              for inputs, targets in get_batches(data, key='eval', batchsize=EVAL_BATCHSIZE):\n",
    "                  if epoch >= ema_epoch_start:\n",
    "                      outputs = net_ema(inputs)\n",
    "                      acc_list_ema.append((outputs.argmax(-1) == targets).float().mean())\n",
    "                  outputs = net(inputs)\n",
    "                  loss_list_val.append(loss_fn(outputs, targets).float().mean())\n",
    "                  acc_list.append((outputs.argmax(-1) == targets).float().mean())\n",
    "                  \n",
    "              val_acc = torch.stack(acc_list).mean().item()\n",
    "              ema_val_acc = None\n",
    "              # TODO: We can fuse these two operations (just above and below) all-together like :D :))))\n",
    "              if epoch >= ema_epoch_start:\n",
    "                  ema_val_acc = torch.stack(acc_list_ema).mean().item()\n",
    "\n",
    "              val_loss = torch.stack(loss_list_val).mean().item()\n",
    "          # We basically need to look up local variables by name so we can have the names, so we can pad to the proper column width.\n",
    "          ## Printing stuff in the terminal can get tricky and this used to use an outside library, but some of the required stuff seemed even\n",
    "          ## more heinous than this, unfortunately. So we switched to the \"more simple\" version of this!\n",
    "          format_for_table = lambda x, locals: (f\"{locals[x]}\".rjust(len(x))) \\\n",
    "                                                    if type(locals[x]) == int else \"{:0.4f}\".format(locals[x]).rjust(len(x)) \\\n",
    "                                                if locals[x] is not None \\\n",
    "                                                else \" \"*len(x)\n",
    "\n",
    "          # Print out our training details (sorry for the complexity, the whole logging business here is a bit of a hot mess once the columns need to be aligned and such....)\n",
    "          ## We also check to see if we're in our final epoch so we can print the 'bottom' of the table for each round.\n",
    "          print_training_details(list(map(partial(format_for_table, locals=locals()), logging_columns_list)), is_final_entry=(epoch == HYP['misc']['train_epochs'] - 1))\n",
    "    return ema_val_acc # Return the final ema accuracy achieved (not using the 'best accuracy' selection strategy, which I think is okay here....)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    acc_list = []\n",
    "    for run_num in range(25):\n",
    "        acc_list.append(torch.tensor(main()))\n",
    "    print(\"Mean and variance:\", (torch.mean(torch.stack(acc_list)).item(), torch.var(torch.stack(acc_list)).item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
