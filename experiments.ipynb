{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as t\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download & Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "DATA_LOCATION = \"data.pt\"\n",
    "\n",
    "# if not os.path.exists(HYP['misc']['data_location']):\n",
    "\n",
    "CIFAR10_MEAN, CIFAR10_STD = [\n",
    "    torch.tensor(\n",
    "        [0.4913997551666284, 0.48215855929893703, 0.4465309133731618], device=DEVICE\n",
    "    ),\n",
    "    torch.tensor(\n",
    "        [0.24703225141799082, 0.24348516474564, 0.26158783926049628], device=DEVICE\n",
    "    ),\n",
    "]\n",
    "\n",
    "PREP_TRANSFORM = t.Compose(\n",
    "    [\n",
    "        TO_TENSOR := t.ToTensor(),\n",
    "        NORMALIZE := t.Normalize(CIFAR10_MEAN, CIFAR10_STD),\n",
    "    ]\n",
    ")\n",
    "\n",
    "TRAIN_TRANSFORM = t.Compose([PAD := t.Pad(BORDER := 4), PREP_TRANSFORM])\n",
    "\n",
    "cifar10 = torchvision.datasets.CIFAR10(\n",
    "    \"cifar10/\", download=True, train=True, transform=TRAIN_TRANSFORM\n",
    ")\n",
    "cifar10_eval = torchvision.datasets.CIFAR10(\n",
    "    \"cifar10/\", download=False, train=False, transform=PREP_TRANSFORM\n",
    ")\n",
    "\n",
    "\n",
    "data = {\"train\": cifar10, \"eval\": cifar10_eval}\n",
    "# torch.save(data, HYP['misc']['data_location'])\n",
    "\n",
    "# else:\n",
    "#     # This is effectively instantaneous, and takes us practically straight to where the dataloader-loaded dataset would be. :)\n",
    "#     # So as long as you run the above loading process once, and keep the file on the disc it's specified by default in the above\n",
    "#     # HYP dictionary, then we should be good. :)\n",
    "#     data = torch.load(HYP['misc']['data_location'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "#          Training Helpers            #\n",
    "########################################\n",
    "\n",
    "\n",
    "def sgd_optimizer(trainable_parameters, weight_decay: float):\n",
    "    return torch.optim.SGD(\n",
    "        trainable_parameters,\n",
    "        weight_decay,\n",
    "        **(SGD_DEFAULT_KWARGS := {\"nesterov\": True, \"momentum\": 0.9}),\n",
    "    )\n",
    "\n",
    "\n",
    "def piecewise_linear_scheduler(\n",
    "    optimizer: torch.optim.Optimizer, epochs: list[int], learning_rates: list[float]\n",
    "):\n",
    "    return torch.optim.lr_scheduler.SequentialLR(\n",
    "        optimizer,\n",
    "        schedulers=[\n",
    "            torch.optim.lr_scheduler.LinearLR(\n",
    "                optimizer,\n",
    "                start_factor=learning_rates[i],\n",
    "                end_factor=learning_rates[i + 1],\n",
    "            )\n",
    "            for i in range(len(learning_rates) - 1)\n",
    "        ],\n",
    "        milestones=epochs[1:-1],\n",
    "    )\n",
    "\n",
    "\n",
    "########################################\n",
    "#           Train and Eval             #\n",
    "########################################\n",
    "\n",
    "\n",
    "def train_test(\n",
    "    model: nn.Module,\n",
    "    criterion,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scheduler: torch.optim.lr_scheduler._LRScheduler,\n",
    "    train_set: torchvision.datasets.CIFAR10,\n",
    "    test_set: torchvision.datasets.CIFAR10,\n",
    "    num_epochs: int,\n",
    "    batch_size: int,\n",
    "    num_workers: int = 0,\n",
    "):\n",
    "    train_loader = DataLoader(\n",
    "        train_set, batch_size, shuffle=True, num_workers=num_workers\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_set, batch_size, shuffle=False, num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch {}\".format(epoch))\n",
    "\n",
    "        train_epoch(model, criterion, optimizer, train_loader)\n",
    "        test_epoch(model, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "def train_epoch(\n",
    "    model: nn.Module,\n",
    "    criterion: nn.modules.loss._Loss,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    train_loader: DataLoader,\n",
    "):\n",
    "    TRANSFORM = t.Compose(\n",
    "        [CROP := t.RandomCrop(IMAGE_SIZE := (32, 32)), FLIP := t.RandomVerticalFlip()]\n",
    "    )\n",
    "\n",
    "    train_correct = 0\n",
    "    train_loss = 0\n",
    "\n",
    "    for i, (batch, targets) in enumerate(train_loader):\n",
    "        batch = TRANSFORM(batch)\n",
    "        batch.to(DEVICE)\n",
    "\n",
    "        output = model(batch)\n",
    "        loss = criterion(output, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        train_correct += pred.eq(targets.view_as(pred)).sum().item()\n",
    "        train_loss += loss\n",
    "\n",
    "        if i % 100 == 10:\n",
    "            print(\n",
    "                \"Train loss {:.4f}, Train accuracy {:.2f}%\".format(\n",
    "                    train_loss / ((i + 1) * 64), 100 * train_correct / ((i + 1) * 64)\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "def test_epoch(model: nn.Module, test_loader: DataLoader):\n",
    "    test_correct = 0\n",
    "    model.eval()\n",
    "\n",
    "    for batch, targets in test_loader:\n",
    "        output = model(batch)\n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        test_correct += pred.eq(targets.view_as(pred)).sum().item()\n",
    "\n",
    "    print(\n",
    "        \"End of testing. Test accuracy {:.2f}%\".format(\n",
    "            100 * test_correct / (len(test_loader) * 64)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "#            Network Components             #\n",
    "#############################################\n",
    "\n",
    "class Cat(nn.Module):\n",
    "    def __init__(self, modules: OrderedDict[str, nn.Module]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        for name, module in modules.items():\n",
    "            self.add_module(name, module)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        return torch.cat([module(x) for module in self.children()])\n",
    "\n",
    "\n",
    "class Add(nn.Module):\n",
    "    def __init__(self, modules: OrderedDict[str, nn.Module]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        for name, module in modules.items():\n",
    "            self.add_module(name, module)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        return sum([module(x) for module in self.children()])\n",
    "\n",
    "\n",
    "class Id(nn.Module):\n",
    "    def forward(self, x: Tensor): return x\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x: Tensor): return x.view(x.size(0), x.size(1))\n",
    "\n",
    "\n",
    "class BatchNorm(nn.BatchNorm2d):\n",
    "    def __init__(self, num_features, weight_requires_grad=True, bias_requires_grad=True, weights_init=False, *args, **kwargs):\n",
    "\n",
    "        super().__init__(num_features, *args, **kwargs)\n",
    "\n",
    "        if weights_init:\n",
    "            self.weight.data.fill_(1.0)\n",
    "            self.bias.data.fill_(0.0)\n",
    "\n",
    "        self.weight.requires_grad = weight_requires_grad\n",
    "        self.bias.requires_grad = bias_requires_grad\n",
    "\n",
    "class Conv(nn.Conv2d):\n",
    "    def __init__(self, in_channels, out_channels, padding=None, *args, **kwargs):\n",
    "\n",
    "        kwargs = {**kwargs, **(DEFAULT_CONV_KWARGS := {'kernel_size': 3, 'padding': 'same', 'bias': False})}\n",
    "\n",
    "        if padding is not None:\n",
    "            kwargs[\"padding\"] = padding\n",
    "\n",
    "        super().__init__(in_channels, out_channels, *args, **kwargs)\n",
    "\n",
    "\n",
    "class ResBlock(nn.Sequential):\n",
    "    def __init__(self, c_in: int, c_out: int, stride: int = 1) -> None:\n",
    "\n",
    "        bn1 = BatchNorm(c_in)\n",
    "        relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        branch = nn.Sequential(OrderedDict([\n",
    "            ('conv1', Conv(c_in, c_out, kernel_size=3,\n",
    "                           stride=stride, padding=1)),\n",
    "            ('bn2', BatchNorm(c_out)),\n",
    "            ('relu2', nn.ReLU(inplace=True)),\n",
    "            ('conv2', Conv(c_out, c_out, kernel_size=3,\n",
    "                           stride=1, padding=1, bias=False)),]))\n",
    "\n",
    "        is_projection_needed = (stride != 1) or (c_in != c_out)\n",
    "\n",
    "        super().__init__(OrderedDict([(\"bn1\", bn1), (\"relu1\", relu1), (\"res\", Add(OrderedDict([\n",
    "            (('conv3', Conv(\n",
    "                c_in, c_out, kernel_size=1, stride=stride, padding=0, bias=False)) if is_projection_needed else (\"id\", Id())),\n",
    "            (\"branch\", branch)\n",
    "        ])))])\n",
    "        )\n",
    "\n",
    "#############################################\n",
    "#            Network Architercture          #\n",
    "#############################################\n",
    "\n",
    "class DawnNet(nn.Sequential):\n",
    "    def __init__(self, c=64, Block=ResBlock, prep_bn_relu=False, concat_pool=False, **kw) -> None:\n",
    "\n",
    "        if isinstance(c, int):\n",
    "            c = [c, 2*c, 4*c, 4*c]\n",
    "\n",
    "        prep = nn.Sequential(OrderedDict(\n",
    "            [('conv', Conv(3, c[0], bias=False))]))\n",
    "\n",
    "        if prep_bn_relu:\n",
    "            prep.add_module('bn', BatchNorm(c[0], **kw))\n",
    "            prep.add_module('relu', nn.ReLU(True))\n",
    "\n",
    "        classifier_pool = Cat(OrderedDict([('maxpool', nn.MaxPool2d(4)),\n",
    "                                           ('avgpool', nn.AvgPool2d(4))\n",
    "                                           ])) if concat_pool else nn.MaxPool2d(4)\n",
    "\n",
    "        super().__init__(OrderedDict([\n",
    "            ('prep', prep),\n",
    "            ('layer1', nn.Sequential(OrderedDict([\n",
    "                ('block0', Block(c[0], c[0], **kw)),\n",
    "                ('block1', Block(c[0], c[0], **kw))\n",
    "            ]))),\n",
    "            ('layer2', nn.Sequential(OrderedDict([\n",
    "                ('block0', Block(c[0], c[1], stride=2, **kw)),\n",
    "                ('block1', Block(c[1], c[1], **kw))\n",
    "            ]))),\n",
    "            ('layer3', nn.Sequential(OrderedDict([\n",
    "                ('block0', Block(c[1], c[2], stride=2, **kw)),\n",
    "                ('block1', Block(c[2], c[2], **kw))\n",
    "            ]))),\n",
    "            ('layer4', nn.Sequential(OrderedDict([\n",
    "                ('block0', Block(c[2], c[3], stride=2, **kw)),\n",
    "                ('block1', Block(c[3], c[3], **kw))\n",
    "            ]))),\n",
    "            ('final', nn.Sequential(OrderedDict([\n",
    "                ('pool', classifier_pool),\n",
    "                ('flatten', Flatten()),\n",
    "                ('linear', nn.Linear(\n",
    "                    (2*c[3] if concat_pool else c[3]), 10, bias=True))\n",
    "            ]))),\n",
    "            ('logits', Id()),\n",
    "        ]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Post 1: Baseline](https://www.myrtle.ai/2018/09/24/how_to_train_your_resnet_1/) - DAWNbench baseline + no initial bn-relu+ efficient dataloading/augmentation, 1 dataloader process (301s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = DawnNet()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = sgd_optimizer(TRAINABLE_PARAMETERS:=net.parameters(), weight_decay=5e-4*(BATCHSIZE := 128))\n",
    "scheduler = piecewise_linear_scheduler(optimizer, \n",
    "                                       EPOCHS:=[0, 15, 30, NUM_EPOCHS:=35], \n",
    "                                       LR:=[0, 0.1, 0.005, 0])\n",
    "\n",
    "# print(net.final)\n",
    "train_test(net, criterion, optimizer, scheduler, data[\"train\"], data[\"eval\"], NUM_EPOCHS, BATCHSIZE, NUM_WORKERS:=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project-hlb-CIFAR10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHSIZE = 512\n",
    "BIAS_SCALER = 32\n",
    "\n",
    "# To replicate the ~95.77% accuracy in 188 seconds runs, simply change the base_depth from 64->128 and the num_epochs from 10->80\n",
    "HYP = {\n",
    "    'opt': {\n",
    "        'bias_lr':        1.15 * 1.35 * 1. * BIAS_SCALER/BATCHSIZE, # TODO: How we're expressing this information feels somewhat clunky, is there maybe a better way to do this? :'))))\n",
    "        'non_bias_lr':    1.15 * 1.35 * 1. / BATCHSIZE,\n",
    "        'bias_decay':     .85 * 4.8e-4 * BATCHSIZE/BIAS_SCALER,\n",
    "        'non_bias_decay': .85 * 4.8e-4 * BATCHSIZE,\n",
    "        'scaling_factor': 1./10,\n",
    "        'percent_start': .2,\n",
    "    },\n",
    "    'net': {\n",
    "        'whitening': {\n",
    "            'kernel_size': 2,\n",
    "            'num_examples': 50000,\n",
    "        },\n",
    "        'batch_norm_momentum': .8,\n",
    "        'cutout_size': 0,\n",
    "        'pad_amount': 3,\n",
    "        'base_depth': 64 ## This should be a factor of 8 in some way to stay tensor core friendly\n",
    "    },\n",
    "    'misc': {\n",
    "        'ema': {\n",
    "            'epochs': 2,\n",
    "            'decay_base': .986,\n",
    "            'every_n_steps': 2,\n",
    "        },\n",
    "        'train_epochs': 10,\n",
    "        'device': DEVICE,\n",
    "        'data_location': DATA_LOCATION,\n",
    "    }\n",
    "}\n",
    "\n",
    "SCALER = 2. ## You can play with this on your own if you want, for the first beta I wanted to keep things simple (for now) and leave it out of the hyperparams dict\n",
    "DEPTHS = {\n",
    "    'init':   round(SCALER**-1*HYP['net']['base_depth']), # 64  w/ scaler at base value\n",
    "    'block1': round(SCALER**1*HYP['net']['base_depth']), # 128 w/ scaler at base value\n",
    "    'block2': round(SCALER**2*HYP['net']['base_depth']), # 256 w/ scaler at base value\n",
    "    'block3': round(SCALER**3*HYP['net']['base_depth']), # 512 w/ scaler at base value\n",
    "    'num_classes': 10\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Init Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patches(x, patch_shape=(3, 3), dtype=torch.float32):\n",
    "    # TODO: Annotate\n",
    "    c, (h, w) = x.shape[1], patch_shape\n",
    "    return x.unfold(2,h,1).unfold(3,w,1).transpose(1,3).reshape(-1,c,h,w).to(dtype) # TODO: Annotate?\n",
    "\n",
    "def get_whitening_parameters(patches):\n",
    "    # TODO: Let's annotate this, please! :'D / D':\n",
    "    n,c,h,w = patches.shape\n",
    "    est_covariance = torch.cov(patches.view(n, c*h*w).t())\n",
    "    eigenvalues, eigenvectors = torch.linalg.eigh(est_covariance, UPLO='U') # this is the same as saying we want our eigenvectors, with the specification that the matrix be an upper triangular matrix (instead of a lower-triangular matrix)\n",
    "    return eigenvalues.flip(0).view(-1, 1, 1, 1), eigenvectors.t().reshape(c*h*w,c,h,w).flip(0)\n",
    "\n",
    "# Run this over the training set to calculate the patch statistics, then set the initial convolution as a non-learnable 'whitening' layer\n",
    "def init_whitening_conv(layer, train_set=None, num_examples=None, previous_block_data=None, pad_amount=0, freeze=True, whiten_splits=None):\n",
    "    if train_set is not None and previous_block_data is None:\n",
    "        if pad_amount > 0:\n",
    "            previous_block_data = train_set[:num_examples,:,pad_amount:-pad_amount,pad_amount:-pad_amount] # if it's none, we're at the beginning of our network.\n",
    "        else:\n",
    "            previous_block_data = train_set[:num_examples,:,:,:]\n",
    "    if whiten_splits is None:\n",
    "         previous_block_data_split = [previous_block_data] # list of length 1 so we can reuse the splitting code down below\n",
    "    else:\n",
    "         previous_block_data_split = previous_block_data.split(whiten_splits, dim=0)\n",
    "\n",
    "    eigenvalue_list, eigenvector_list = [], []\n",
    "    for data_split in previous_block_data_split:\n",
    "        eigenvalues, eigenvectors = get_whitening_parameters(get_patches(data_split, patch_shape=layer.weight.data.shape[2:])) # center crop to remove padding\n",
    "        eigenvalue_list.append(eigenvalues)\n",
    "        eigenvector_list.append(eigenvectors)\n",
    "\n",
    "    eigenvalues = torch.stack(eigenvalue_list, dim=0).mean(0)\n",
    "    eigenvectors = torch.stack(eigenvector_list, dim=0).mean(0)\n",
    "    # for some reason, the eigenvalues and eigenvectors seem to come out all in float32 for this? ! ?! ?!?!?!? :'(((( </3\n",
    "    set_whitening_conv(layer, eigenvalues.to(dtype=layer.weight.dtype), eigenvectors.to(dtype=layer.weight.dtype), freeze=freeze)\n",
    "    data = layer(previous_block_data.to(dtype=layer.weight.dtype))\n",
    "    return data\n",
    "\n",
    "def set_whitening_conv(conv_layer, eigenvalues, eigenvectors, eps=1e-2, freeze=True):\n",
    "    shape = conv_layer.weight.data.shape\n",
    "    conv_layer.weight.data[-eigenvectors.shape[0]:, :, :, :] = (eigenvectors/torch.sqrt(eigenvalues+eps))[-shape[0]:, :, :, :]\n",
    "    ## We don't want to train this, since this is implicitly whitening over the whole dataset\n",
    "    ## For more info, see David Page's original blogposts (link in the README.md as of this commit.)\n",
    "    if freeze: \n",
    "        conv_layer.weight.requires_grad = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "#         (EXTRA) Network Components        #\n",
    "#############################################\n",
    "\n",
    "# can hack any changes to each residual group that you want directly in here\n",
    "class ConvGroup(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out, residual, short, pool, se):\n",
    "        super().__init__()\n",
    "        self.short = short\n",
    "        self.pool = pool\n",
    "        self.se = se\n",
    "\n",
    "        self.residual = residual\n",
    "        self.channels_in = channels_in\n",
    "        self.channels_out = channels_out\n",
    "\n",
    "        self.conv1 = Conv(channels_in, channels_out)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.norm1 = BatchNorm(channels_out)\n",
    "        self.activ = nn.GELU()          \n",
    "\n",
    "        if not short:\n",
    "            self.conv2 = Conv(channels_out, channels_out)\n",
    "            self.conv3 = Conv(channels_out, channels_out)\n",
    "            self.norm2 = BatchNorm(channels_out)\n",
    "            self.norm3 = BatchNorm(channels_out)\n",
    "\n",
    "            self.se1 = nn.Linear(channels_out, channels_out//16)\n",
    "            self.se2 = nn.Linear(channels_out//16, channels_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        if self.pool:\n",
    "            x = self.pool1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.activ(x)\n",
    "        if self.short: # layer 2 doesn't necessarily need the residual, so we just return it.\n",
    "            return x\n",
    "        residual = x\n",
    "        if self.se:\n",
    "            mult = torch.sigmoid(self.se2(self.activ(self.se1(torch.mean(residual, dim=(2,3)))))).unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.activ(x)\n",
    "        x = self.conv3(x)\n",
    "        \n",
    "        if self.se:\n",
    "            x = x * mult\n",
    "\n",
    "        x = self.norm3(x)\n",
    "        x = self.activ(x)\n",
    "        x = x + residual # haiku\n",
    "\n",
    "        return x\n",
    "\n",
    "# Set to 1 for now just to debug a few things....\n",
    "class TemperatureScaler(nn.Module):\n",
    "    def __init__(self, init_val):\n",
    "        super().__init__()\n",
    "        self.scaler = torch.tensor(init_val)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x.float() ## save precision for the gradients in the backwards pass\n",
    "                  ## I personally believe from experience that this is important\n",
    "                  ## for a few reasons. I believe this is the main functional difference between\n",
    "                  ## my implementation, and David's implementation...\n",
    "        return x.mul(self.scaler)\n",
    "\n",
    "class FastGlobalMaxPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Previously was chained torch.max calls.\n",
    "        # requires less time than AdaptiveMax2dPooling -- about ~.3s for the entire run, in fact (which is pretty significant! :O :D :O :O <3 <3 <3 <3)\n",
    "        return torch.amax(x, dim=(2,3)) # Global maximum pooling\n",
    "    \n",
    "#############################################\n",
    "#            Network Definition             #\n",
    "#############################################\n",
    "    \n",
    "class SpeedyResNet(nn.Module):\n",
    "    def __init__(self, network_dict):\n",
    "        super().__init__()\n",
    "        self.net_dict = network_dict # flexible, defined in the make_net function\n",
    "\n",
    "    # This allows you to customize/change the execution order of the network as needed.\n",
    "    def forward(self, x):\n",
    "        if not self.training:\n",
    "            x = torch.cat((x, torch.flip(x, (-1,))))\n",
    "        x = self.net_dict['initial_block']['whiten'](x)\n",
    "        x = self.net_dict['initial_block']['project'](x)\n",
    "        x = self.net_dict['initial_block']['norm'](x)\n",
    "        x = self.net_dict['initial_block']['activation'](x)\n",
    "        x = self.net_dict['residual1'](x)\n",
    "        x = self.net_dict['residual2'](x)\n",
    "        x = self.net_dict['residual3'](x)\n",
    "        x = self.net_dict['pooling'](x)\n",
    "        x = self.net_dict['linear'](x)\n",
    "        x = self.net_dict['temperature'](x)\n",
    "        if not self.training:\n",
    "            # Average the predictions from the lr-flipped inputs during eval\n",
    "            orig, flipped = x.split(x.shape[0]//2, dim=0)\n",
    "            x = .5 * orig + .5 * flipped\n",
    "        return x\n",
    "    \n",
    "def make_net():\n",
    "    # TODO: A way to make this cleaner??\n",
    "    # Note, you have to specify any arguments overlapping with defaults (i.e. everything but in/out depths) as kwargs so that they are properly overridden (TODO cleanup somehow?)\n",
    "    whiten_conv_depth = 3*HYP['net']['whitening']['kernel_size']**2\n",
    "    network_dict = nn.ModuleDict({\n",
    "        'initial_block': nn.ModuleDict({\n",
    "            'whiten': Conv(3, whiten_conv_depth, kernel_size=HYP['net']['whitening']['kernel_size'], padding=0),\n",
    "            'project': Conv(whiten_conv_depth, DEPTHS['init'], kernel_size=1),\n",
    "            'norm': BatchNorm(DEPTHS['init'], weight=False),\n",
    "            'activation': nn.GELU(),\n",
    "        }),\n",
    "        'residual1': ConvGroup(DEPTHS['init'], DEPTHS['block1'], residual=True, short=False, pool=True, se=True),\n",
    "        'residual2': ConvGroup(DEPTHS['block1'], DEPTHS['block2'], residual=True, short=True, pool=True, se=True),\n",
    "        'residual3': ConvGroup(DEPTHS['block2'], DEPTHS['block3'], residual=True, short=False, pool=True, se=True),\n",
    "        'pooling': FastGlobalMaxPooling(),\n",
    "        'linear': nn.Linear(DEPTHS['block3'], DEPTHS['num_classes'], bias=False),\n",
    "        'temperature': TemperatureScaler(HYP['opt']['scaling_factor'])\n",
    "    })\n",
    "\n",
    "    net = SpeedyResNet(network_dict)\n",
    "    net = net.to(HYP['misc']['device'])\n",
    "    net = net.to(memory_format=torch.channels_last) # to appropriately use tensor cores/avoid thrash while training\n",
    "    net.train()\n",
    "    net.half() # Convert network to half before initializing the initial whitening layer.\n",
    "\n",
    "    ## Initialize the whitening convolution\n",
    "    with torch.no_grad():\n",
    "        # Initialize the first layer to be fixed weights that whiten the expected input values of the network be on the unit HYPersphere. (i.e. their...average vector length is 1.?, IIRC)\n",
    "        init_whitening_conv(net.net_dict['initial_block']['whiten'],\n",
    "                            data['train']['images'].index_select(0, torch.randperm(data['train']['images'].shape[0], device=data['train']['images'].device)),\n",
    "                            num_examples=HYP['net']['whitening']['num_examples'],\n",
    "                            pad_amount=HYP['net']['pad_amount'],\n",
    "                            whiten_splits=5000) ## Hardcoded for now while we figure out the optimal whitening number\n",
    "                                                ## If you're running out of memory (OOM) feel free to decrease this, but\n",
    "                                                ## the index lookup in the dataloader may give you some trouble depending\n",
    "                                                ## upon exactly how memory-limited you are\n",
    "\n",
    "    return net"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing (EXTRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is actually (I believe) a pretty clean implementation of how to do something like this, since shifted-square masks unique to each depth-channel can actually be rather\n",
    "## tricky in practice. That said, if there's a better way, please do feel free to submit it! This can be one of the harder parts of the code to understand (though I personally get\n",
    "## stuck on the fold/unfold process for the lower-level convolution calculations.\n",
    "def make_random_square_masks(inputs, mask_size):\n",
    "    ##### TODO: Double check that this properly covers the whole range of values. :'( :')\n",
    "    if mask_size == 0:\n",
    "        return None # no need to cutout or do anything like that since the patch_size is set to 0\n",
    "    is_even = int(mask_size % 2 == 0)\n",
    "    in_shape = inputs.shape\n",
    "\n",
    "    # seed centers of squares to cutout boxes from, in one dimension each\n",
    "    mask_center_y = torch.empty(in_shape[0], dtype=torch.long, device=inputs.device).random_(mask_size//2-is_even, in_shape[-2]-mask_size//2-is_even)\n",
    "    mask_center_x = torch.empty(in_shape[0], dtype=torch.long, device=inputs.device).random_(mask_size//2-is_even, in_shape[-1]-mask_size//2-is_even)\n",
    "\n",
    "    # measure distance, using the center as a reference point\n",
    "    to_mask_y_dists = torch.arange(in_shape[-2], device=inputs.device).view(1, 1, in_shape[-2], 1) - mask_center_y.view(-1, 1, 1, 1)\n",
    "    to_mask_x_dists = torch.arange(in_shape[-1], device=inputs.device).view(1, 1, 1, in_shape[-1]) - mask_center_x.view(-1, 1, 1, 1)\n",
    "\n",
    "    to_mask_y = (to_mask_y_dists >= (-(mask_size // 2) + is_even)) * (to_mask_y_dists <= mask_size // 2)\n",
    "    to_mask_x = (to_mask_x_dists >= (-(mask_size // 2) + is_even)) * (to_mask_x_dists <= mask_size // 2)\n",
    "\n",
    "    final_mask = to_mask_y * to_mask_x ## Turn (y by 1) and (x by 1) boolean masks into (y by x) masks through multiplication. Their intersection is square, hurray! :D\n",
    "\n",
    "    return final_mask\n",
    "\n",
    "def batch_cutout(inputs, patch_size):\n",
    "    with torch.no_grad():\n",
    "        cutout_batch_mask = make_random_square_masks(inputs, patch_size)\n",
    "        if cutout_batch_mask is None:\n",
    "            return inputs # if the mask is None, then that's because the patch size was set to 0 and we will not be using cutout today.\n",
    "        # TODO: Could be fused with the crop operation for sheer speeeeeds. :D <3 :))))\n",
    "        cutout_batch = torch.where(cutout_batch_mask, torch.zeros_like(inputs), inputs)\n",
    "        return cutout_batch\n",
    "    \n",
    "def batch_crop(inputs, crop_size):\n",
    "    with torch.no_grad():\n",
    "        crop_mask_batch = make_random_square_masks(inputs, crop_size)\n",
    "        cropped_batch = torch.masked_select(inputs, crop_mask_batch).view(inputs.shape[0], inputs.shape[1], crop_size, crop_size)\n",
    "        return cropped_batch\n",
    "\n",
    "def batch_flip_lr(batch_images, flip_chance=.5):\n",
    "    with torch.no_grad():\n",
    "        # TODO: Is there a more elegant way to do this? :') :'((((\n",
    "        return torch.where(torch.rand_like(batch_images[:, 0, 0, 0].view(-1, 1, 1, 1)) < flip_chance, torch.flip(batch_images, (-1,)), batch_images)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Helpers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "class NetworkEMA(nn.Module):\n",
    "    def __init__(self, net, decay):\n",
    "        super().__init__() # init the parent module so this module is registered properly\n",
    "        self.net_ema = copy.deepcopy(net).eval().requires_grad_(False) # copy the model\n",
    "        self.decay = decay ## you can update/hack this as necessary for update scheduling purposes :3\n",
    "\n",
    "    def update(self, current_net):\n",
    "        with torch.no_grad():\n",
    "            for ema_net_parameter, incoming_net_parameter in zip(self.net_ema.state_dict().values(), current_net.state_dict().values()): # potential bug: assumes that the network architectures don't change during training (!!!!)\n",
    "                if incoming_net_parameter.dtype in (torch.half, torch.float):\n",
    "                    ema_net_parameter.mul_(self.decay).add_(incoming_net_parameter.detach().mul(1. - self.decay)) # update the ema values in place, similar to how optimizer momentum is coded\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        with torch.no_grad():\n",
    "            return self.net_ema(inputs)\n",
    "\n",
    "# TODO: Could we jit this in the (more distant) future? :)\n",
    "@torch.no_grad()\n",
    "def get_batches(data_dict, key, batchsize):\n",
    "    num_epoch_examples = len(data_dict[key]['images'])\n",
    "    shuffled = torch.randperm(num_epoch_examples, device='cuda')\n",
    "    crop_size = 32\n",
    "    ## Here, we prep the dataset by applying all data augmentations in batches ahead of time before each epoch, then we return an iterator below\n",
    "    ## that iterates in chunks over with a random derangement (i.e. shuffled indices) of the individual examples. So we get perfectly-shuffled\n",
    "    ## batches (which skip the last batch if it's not a full batch), but everything seems to be (and hopefully is! :D) properly shuffled. :)\n",
    "    if key == 'train':\n",
    "        images = batch_crop(data_dict[key]['images'], crop_size) # TODO: hardcoded image size for now?\n",
    "        images = batch_flip_lr(images)\n",
    "        images = batch_cutout(images, patch_size=HYP['net']['cutout_size'])\n",
    "    else:\n",
    "        images = data_dict[key]['images']\n",
    "\n",
    "    # Send the images to an (in beta) channels_last to help improve tensor core occupancy (and reduce NCHW <-> NHWC thrash) during training\n",
    "    images = images.to(memory_format=torch.channels_last)\n",
    "    for idx in range(num_epoch_examples // batchsize):\n",
    "        if not (idx+1)*batchsize > num_epoch_examples: ## Use the shuffled randperm to assemble individual items into a minibatch\n",
    "            yield images.index_select(0, shuffled[idx*batchsize:(idx+1)*batchsize]), \\\n",
    "                  data_dict[key]['targets'].index_select(0, shuffled[idx*batchsize:(idx+1)*batchsize]) ## Each item is only used/accessed by the network once per epoch. :D\n",
    "\n",
    "\n",
    "def init_split_parameter_dictionaries(network):\n",
    "    params_non_bias = {'params': [], 'lr': HYP['opt']['non_bias_lr'], 'momentum': .85, 'nesterov': True, 'weight_decay': HYP['opt']['non_bias_decay']}\n",
    "    params_bias     = {'params': [], 'lr': HYP['opt']['bias_lr'],     'momentum': .85, 'nesterov': True, 'weight_decay': HYP['opt']['bias_decay']}\n",
    "\n",
    "    for name, p in network.named_parameters():\n",
    "        if p.requires_grad:\n",
    "            if 'bias' in name:\n",
    "                params_bias['params'].append(p)\n",
    "            else:\n",
    "                params_non_bias['params'].append(p)\n",
    "    return params_non_bias, params_bias\n",
    "\n",
    "\n",
    "## Hey look, it's the soft-targets/label-smoothed loss! Native to PyTorch. Now, _that_ is pretty cool, and simplifies things a lot, to boot! :D :)\n",
    "loss_fn = nn.CrossEntropyLoss(label_smoothing=0.2, reduction='none')\n",
    "\n",
    "logging_columns_list = ['epoch', 'train_loss', 'val_loss', 'train_acc', 'val_acc', 'ema_val_acc', 'total_time_seconds']\n",
    "# define the printing function and print the column heads\n",
    "def print_training_details(columns_list, separator_left='|  ', separator_right='  ', final=\"|\", column_heads_only=False, is_final_entry=False):\n",
    "    print_string = \"\"\n",
    "    if column_heads_only:\n",
    "        for column_head_name in columns_list:\n",
    "            print_string += separator_left + column_head_name + separator_right\n",
    "        print_string += final\n",
    "        print('-'*(len(print_string))) # print the top bar\n",
    "        print(print_string)\n",
    "        print('-'*(len(print_string))) # print the bottom bar\n",
    "    else:\n",
    "        for column_value in columns_list:\n",
    "            print_string += separator_left + column_value + separator_right\n",
    "        print_string += final\n",
    "        print(print_string)\n",
    "    if is_final_entry:\n",
    "        print('-'*(len(print_string))) # print the final output bar\n",
    "\n",
    "print_training_details(logging_columns_list, column_heads_only=True) ## print out the training column heads before we print the actual content for each run.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train & Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Initializing constants for the whole run.\n",
    "    net_ema = None ## Reset any existing network emas, we want to have _something_ to check for existence so we can initialize the EMA right from where the network is during training\n",
    "                   ## (as opposed to initializing the network_ema from the randomly-initialized starter network, then forcing it to play catch-up all of a sudden in the last several epochs)\n",
    "\n",
    "    total_time_seconds = 0.\n",
    "    current_steps = 0.\n",
    "    \n",
    "    # TODO: Doesn't currently account for partial epochs really (since we're not doing \"real\" epochs across the whole BATCHSIZE)....\n",
    "    num_steps_per_epoch      = len(data['train']['images']) // BATCHSIZE\n",
    "    total_train_steps        = num_steps_per_epoch * HYP['misc']['train_epochs']\n",
    "    ema_epoch_start          = HYP['misc']['train_epochs'] - HYP['misc']['ema']['epochs']\n",
    "    num_cooldown_before_freeze_steps = 0\n",
    "    num_low_lr_steps_for_ema = HYP['misc']['ema']['epochs'] * num_steps_per_epoch\n",
    "\n",
    "    ## I believe this wasn't logged, but the EMA update power is adjusted by being raised to the power of the number of \"every n\" steps\n",
    "    ## to somewhat accomodate for whatever the expected information intake rate is. The tradeoff I believe, though, is that this is to some degree noisier as we\n",
    "    ## are intaking fewer samples of our distribution-over-time, with a higher individual weight each. This can be good or bad depending upon what we want.\n",
    "    projected_ema_decay_val  = HYP['misc']['ema']['decay_base'] ** HYP['misc']['ema']['every_n_steps']\n",
    "\n",
    "    # Adjust pct_start based upon how many epochs we need to finetune the ema at a low lr for\n",
    "    pct_start = HYP['opt']['percent_start'] * (total_train_steps/(total_train_steps - num_low_lr_steps_for_ema))\n",
    "\n",
    "    # Get network\n",
    "    net = make_net()\n",
    "\n",
    "    ## Stowing the creation of these into a helper function to make things a bit more readable....\n",
    "    non_bias_params, bias_params = init_split_parameter_dictionaries(net)\n",
    "\n",
    "    # One optimizer for the regular network, and one for the biases. This allows us to use the superconvergence onecycle training policy for our networks....\n",
    "    opt = torch.optim.SGD(**non_bias_params)\n",
    "    opt_bias = torch.optim.SGD(**bias_params)\n",
    "\n",
    "    #opt = torch.optim.SGD(**non_bias_params)\n",
    "    #opt_bias = torch.optim.SGD(**bias_params)\n",
    "\n",
    "    ## Not the most intuitive, but this basically takes us from ~0 to max_lr at the point pct_start, then down to .1 * max_lr at the end (since 1e16 * 1e-15 = .1 --\n",
    "    ##   This quirk is because the final lr value is calculated from the starting lr value and not from the maximum lr value set during training)\n",
    "    initial_div_factor = 1e16 # basically to make the initial lr ~0 or so :D\n",
    "    final_lr_ratio = .135\n",
    "    lr_sched      = torch.optim.lr_scheduler.OneCycleLR(opt,  max_lr=non_bias_params['lr'], pct_start=pct_start, div_factor=initial_div_factor, final_div_factor=1./(initial_div_factor*final_lr_ratio), total_steps=total_train_steps-num_low_lr_steps_for_ema, anneal_strategy='linear', cycle_momentum=False)\n",
    "    lr_sched_bias = torch.optim.lr_scheduler.OneCycleLR(opt_bias, max_lr=bias_params['lr'], pct_start=pct_start, div_factor=initial_div_factor, final_div_factor=1./(initial_div_factor*final_lr_ratio), total_steps=total_train_steps-num_low_lr_steps_for_ema, anneal_strategy='linear', cycle_momentum=False)\n",
    "\n",
    "    ## For accurately timing GPU code\n",
    "    starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "    ## There's another repository that's mainly reorganized David's code while still maintaining some of the functional structure, and it\n",
    "    ## has a timing feature too, but there's no synchronizes so I suspect the times reported are much faster than they may be in actuality\n",
    "    ## due to some of the quirks of timing GPU operations.\n",
    "    torch.cuda.synchronize() ## clean up any pre-net setup operations\n",
    "    \n",
    "\n",
    "    if True: ## Sometimes we need a conditional/for loop here, this is placed to save the trouble of needing to indent\n",
    "        for epoch in range(HYP['misc']['train_epochs']):\n",
    "          #################\n",
    "          # Training Mode #\n",
    "          #################\n",
    "          torch.cuda.synchronize()\n",
    "          starter.record()\n",
    "          net.train()\n",
    "\n",
    "          loss_train = None\n",
    "          accuracy_train = None\n",
    "\n",
    "          for epoch_step, (inputs, targets) in enumerate(get_batches(data, key='train', batchsize=BATCHSIZE)):\n",
    "              ## Run everything through the network\n",
    "              outputs = net(inputs)\n",
    "              \n",
    "              loss_scale_scaler = 1./16 # Hardcoded for now, preserves some accuracy during the loss summing process, balancing out its regularization effects\n",
    "              ## If you want to add other losses or hack around with the loss, you can do that here.\n",
    "              loss = loss_fn(outputs, targets).mul(loss_scale_scaler).sum().div(loss_scale_scaler) ## Note, as noted in the original blog posts, the summing here does a kind of loss scaling\n",
    "                                                     ## (and is thus BATCHSIZE dependent as a result). This can be somewhat good or bad, depending...\n",
    "\n",
    "              # we only take the last-saved accs and losses from train\n",
    "              if epoch_step % 50 == 0:\n",
    "                  train_acc = (outputs.detach().argmax(-1) == targets).float().mean().item()\n",
    "                  train_loss = loss.detach().cpu().item()/BATCHSIZE\n",
    "\n",
    "              loss.backward()\n",
    "\n",
    "              ## Step for each optimizer, in turn.\n",
    "              opt.step()\n",
    "              opt_bias.step()\n",
    "\n",
    "              if current_steps < total_train_steps - num_low_lr_steps_for_ema - 1: # the '-1' is because the lr scheduler tends to overshoot (even below 0 if the final lr is ~0) on the last step for some reason.\n",
    "                  # We only want to step the lr_schedulers while we have training steps to consume. Otherwise we get a not-so-friendly error from PyTorch\n",
    "                  lr_sched.step()\n",
    "                  lr_sched_bias.step()\n",
    "\n",
    "              ## Using 'set_to_none' I believe is slightly faster (albeit riskier w/ funky gradient update workflows) than under the default 'set to zero' method\n",
    "              opt.zero_grad(set_to_none=True)\n",
    "              opt_bias.zero_grad(set_to_none=True)\n",
    "              current_steps += 1\n",
    "\n",
    "              if epoch >= ema_epoch_start and current_steps % HYP['misc']['ema']['every_n_steps'] == 0:          \n",
    "                  ## Initialize the ema from the network at this point in time if it does not already exist.... :D\n",
    "                  if net_ema is None or epoch_step < num_cooldown_before_freeze_steps: # don't snapshot the network yet if so!\n",
    "                      net_ema = NetworkEMA(net, decay=projected_ema_decay_val)\n",
    "                      continue\n",
    "                  net_ema.update(net)\n",
    "          ender.record()\n",
    "          torch.cuda.synchronize()\n",
    "          total_time_seconds += 1e-3 * starter.elapsed_time(ender)\n",
    "\n",
    "          ####################\n",
    "          # Evaluation  Mode #\n",
    "          ####################\n",
    "          net.eval()\n",
    "\n",
    "          EVAL_BATCHSIZE = 1000\n",
    "          assert data['eval']['images'].shape[0] % EVAL_BATCHSIZE == 0, \"Error: The eval BATCHSIZE must evenly divide the eval dataset (for now, we don't have drop_remainder implemented yet).\"\n",
    "          loss_list_val, acc_list, acc_list_ema = [], [], []\n",
    "          \n",
    "          with torch.no_grad():\n",
    "              for inputs, targets in get_batches(data, key='eval', batchsize=EVAL_BATCHSIZE):\n",
    "                  if epoch >= ema_epoch_start:\n",
    "                      outputs = net_ema(inputs)\n",
    "                      acc_list_ema.append((outputs.argmax(-1) == targets).float().mean())\n",
    "                  outputs = net(inputs)\n",
    "                  loss_list_val.append(loss_fn(outputs, targets).float().mean())\n",
    "                  acc_list.append((outputs.argmax(-1) == targets).float().mean())\n",
    "                  \n",
    "              val_acc = torch.stack(acc_list).mean().item()\n",
    "              ema_val_acc = None\n",
    "              # TODO: We can fuse these two operations (just above and below) all-together like :D :))))\n",
    "              if epoch >= ema_epoch_start:\n",
    "                  ema_val_acc = torch.stack(acc_list_ema).mean().item()\n",
    "\n",
    "              val_loss = torch.stack(loss_list_val).mean().item()\n",
    "          # We basically need to look up local variables by name so we can have the names, so we can pad to the proper column width.\n",
    "          ## Printing stuff in the terminal can get tricky and this used to use an outside library, but some of the required stuff seemed even\n",
    "          ## more heinous than this, unfortunately. So we switched to the \"more simple\" version of this!\n",
    "          format_for_table = lambda x, locals: (f\"{locals[x]}\".rjust(len(x))) \\\n",
    "                                                    if type(locals[x]) == int else \"{:0.4f}\".format(locals[x]).rjust(len(x)) \\\n",
    "                                                if locals[x] is not None \\\n",
    "                                                else \" \"*len(x)\n",
    "\n",
    "          # Print out our training details (sorry for the complexity, the whole logging business here is a bit of a hot mess once the columns need to be aligned and such....)\n",
    "          ## We also check to see if we're in our final epoch so we can print the 'bottom' of the table for each round.\n",
    "          print_training_details(list(map(partial(format_for_table, locals=locals()), logging_columns_list)), is_final_entry=(epoch == HYP['misc']['train_epochs'] - 1))\n",
    "    return ema_val_acc # Return the final ema accuracy achieved (not using the 'best accuracy' selection strategy, which I think is okay here....)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    acc_list = []\n",
    "    for run_num in range(25):\n",
    "        acc_list.append(torch.tensor(main()))\n",
    "    print(\"Mean and variance:\", (torch.mean(torch.stack(acc_list)).item(), torch.var(torch.stack(acc_list)).item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
